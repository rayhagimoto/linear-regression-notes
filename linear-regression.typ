#import "@preview/physica:0.9.4"
#import "/global-templates/lib.typ" : *
#import "/global-templates/template.typ" : *

#show: template

// Macros
#let xbar = $overline(x)$
#let ybar = $overline(y)$

// Document
#make-title("Linear Regression")

#block(width:100%)[
  #set quote(block: true)
  #quote(attribution: [Albert Einstein])["Everything should be made as simple as possible, but not simpler."]
]

= Introduction

The most general relationship between variables $x$ and $y$ is a statistical one. 
Every data point $(x,y)$ is generated by sampling from the joint distribution between $x$ and $y$, denoted by $p(x,y)$.
It is useful to write this relationship in terms of the distribution of $y$ conditioned on $x$, since often we care about predicting $y$ given observations of $x$. 
We therefore write

// #bluebox[
  $ (x,y) &~ p(y|x) thin p(x) med , #<eq:conditional-sample> $
// ]

where $p(x)$ is the marginal distribution of $x$.
In general we want to learn $p(y|x)$ from observed data $cal(D) equiv {(x_i, y_i) : i = 1, dots, N}$.
However, we are often limited to learning the conditional mean $EE[y|x]$ (as in the case of minimising an $L_2$ loss), or median (as in the case of minimising an $L_1$ loss).


== Linear model

The simplest model is a linear one that assumes $y$ depends linearly on the model parameters $beta$. One example, for the univariate case is
$ y = beta_0 + beta_1 x + epsilon med , $ <eq:reg-model>
where $epsilon$ is the residual, a random variable which captures the uncertainty in measurements of $y$. 
Another example is 

$ y = beta_0 + beta_1 x^2 + epsilon med $
The only difference is that $x$ has been replaced with $x^2$, which makes the model non-linear in $x$. However, since the model is still linear in $y$ and the model parameters $beta_0, beta_1$, this is still considered a linear model. *Linearity, in this context, means linear w.r.t $y$ and $beta$*. 

Without loss of generality we take #eref(<eq:reg-model>) to be our model.
Having chosen a model the next obvious question is how we fit the model parameters (in this case $beta_0$ and $beta_1$) given some data? 
A common approach is to do _ordinary least squares (OLS)_ regression, where one quantifies the performance of a set of parameters by the sum of squared differences between predictions and observed values, $L = sum_i [y_i - beta_0 - beta_1 x_i]^2$.

It is certainly reasonable to consider this loss function, but why not the sum of absolute values or sum of 4-th power residuals, or something else entirely? Does it even matter? It turns out it does matter. Since it matters, it's important to motivate this loss function to see what implicit assumptions are being made.
We will do this in the next section.

== Deriving the least-squares loss

We start by specifying the conditional distribution $f(y|x)$. Given $x$, the randomness in $y$ is sourced by the residual $epsilon$. If we assume $epsilon ~ N(0, sigma^2)$ then for a single observation we get the log-likelihood
$
  - 2 log cal(L)(y|x,beta) = 1 / (sigma^2) (y - beta_0 - beta_1 x)^2 med + "constants" .
$
When we have $N$ data this becomes
$
  - 2 log cal(L)(y|x,beta) = 1 / (sigma^2) sum_(i=1)^N (y_i - beta_0 - beta_1 x_i)^2 med .
$ <eq:least-square>
On the lhs of #eref(<eq:least-square>) $y$ represents the collection $(y_1, y_2, dots, y_N)$, and similarly for $x$. 

We can derive statistical estimators for $beta_0$ and $beta_1$ by finding the values where they maximise the likelihood, or equivalently, minimise the negative log-likelihood. 
From #eref(<eq:least-square>) we identify that the loss given by the negative loss-likelihood is precisely the least-squares loss function.

In other words, *assuming Gaussian residuals $epsilon_i ~ N(0, sigma^2)$ leads to the least squares loss*.


== Aside: what do we learn by minimising the least-squares loss function?

Suppose we have a flexible model $f(x;theta)$ with parameters $theta$ that we wish to train to predict $y$ given measurements of $x$.
If we identify the best-fit parameters $theta^*$ as those that minimise the squared difference between our model and true values on our dataset $cal(D)$.
That is, by minimising
$
  L[f] =  sum_i [y_i - f(x_i;theta)]^2 med ,
$
where I've written $L[f]$ to emphasize that the loss function can be interpreted as a functional in terms of the model $f$.
In the limit of infinite data the sum over $i$ is just an average over the joint distribution $f(x,y)$.
$
  L[f]
  &-> 
  integral.double [y - f(x;theta)]^2 p(x,y) dif x dif y #no-num\ 
  &=  
  integral.double [y - f(x;theta)]^2 p(y|x) p(x) dif x dif y #<eq:ls-general>
$
Now we minimise $L$ by varing $f$ (we could equivalently varying $theta$, but doing it this way is more clean, and more fun). 
Setting $delta L = 0$ yields
$
 (delta L) / (delta f) &= integral (y - f(x;theta)) thin p(y|x) thin p(x) dif y = (EE[y|x] - f(x;theta)) thin p(x) = 0 #no-num\
 &=> f(x;theta^*) = EE[y|x] med.
$
This is an important result. 
It tells us that even if we have _infinite data_ and an _arbitrarily flexible model_, *the best we can do by minimising a least-squares loss is to learn the conditional expectation of $y$ given $x$*.

_Note: A really nice reference for the content in this section is the introduction of ref._ @bishop1994mixture.

#pagebreak()
== OLS estimators

The maximum likelihood estimator is obtained by taking the derivative of #eref(<eq:least-square>) w.r.t. to $beta_0$ and $beta_1$, setting their results equal to zero, and solving for $beta_0$ and $beta_1$. 
The results are simply,

#bluebox[
$
  hat(beta)_1 &= (sum_i (x_i - xbar)(y_i - ybar)) / (sum_i (x_i - xbar^&)^2) = S_(x y) / S_(x x) = rho_(x y) S_(y y) / S_(x x), #<eq:beta-0-est> \
  
  hat(beta)_0 &= ybar - hat(beta)_1 xbar med, #<eq:beta-1-est> 
$
]
where I have introduced the estimators for standard error $S$ and correlation $rho$,
$
  S_(x y)^2 equiv 1 / (n-1) sum_i (x_i - xbar)(y_i - ybar) \ 
  rho_(x y) equiv S_(x y) / (S_(x x) S_(y y)) med ,
$
and overlines denote sample means, e.g. $xbar = 1 / N sum_(i=1)^N x_i$.

== Properties of estimators 

_Unbiasedness_\
When conditioned on $x$ we can show the estimators are unbiased. In the following when I write $EE(y)$ I mean the expectation of $y$ conditioned on $x$. Moreover, the expectation of any arbitrary function of $x = (x_1, x_2, dots, x_N)$ is itself when conditioned on $x$.
$
  EE hat(beta)_1 
  &= (sum_i (x_i - xbar) EE (y_i - ybar)) / (sum_i (x_i - xbar)^2) med ,
$
but,
$
  EE(y_i - ybar) = (beta_0 + beta_1 x_i - beta_0 - beta_1 xbar) = beta_1(x_i - xbar) med .
$
And so,
$
  EE hat(beta)_1 = beta_1 med .
$
Thus, for $hat(beta)_0 = ybar - hat(beta)_1 xbar$ we have 
$
  EE hat(beta)_0 = EE ybar - xbar thin EE hat(beta)_1 = beta_0 + beta_1 xbar - xbar hat(beta)_1 = beta_0 med .
$

_Variance_\

I'll just state the results, because it is tedious.\ 
#bluebox[
$
  Var(hat(beta)_1) = sigma^2 / (sum_i (x_i - xbar)^2) \
  Var(hat(beta)_0) = (sigma^2 sum_i x_i^2) / (n sum_i (x_i - xbar)^2)
$
]

== Signifcance testing

The linear correlation between $x$ and $y$ is typically assessed via the $t$-statistic,
$
  hat(t) = hat(beta)_1 / (hat(sigma) slash S_(x x) ) med ,
$
where $hat(sigma)$ is the estimator for the standard deviation of the residuals and is given by
$
  hat(sigma) = sum_i (y_i - hat(beta)_0 - hat(beta)_1 x_i )^2 med .
$
If the $epsilon_i$ are assumed to (1) be Gaussian with mean zero (2) have no autocorrelation (3) exhibit weak exogeneity, then the $t$-statistic follows a $t$ distribution which can be used to calculate $p$-values for significance testing. 
However, if any of these assumptions are violated you can't use the standard $p$-values. 
This happens basically all the time in financial time series analysis where, for example, you may model the next time step $y_t$ as a linear combination of lagged values.
This introduces autocorrelation in the residuals.
The Dickey-Fuller test takes this into account when calculating $p$-values for the presence of a unit root.

== Multiple regressors

Suppose we want to use $p$ covariates to predict the variate $y$. We can write down this model as
$
  y_i = beta_0 + sum_(k=1)^p x_(k,i) beta_k  + epsilon_k "for" i = 1, 2, dots, N. #<eq:multiple-lr-model>
$
Or, if we define $X in RR^(N times (p + 1))$  as the matrix 
$
  X equiv mat(
    1, x_(1,1), x_(2,1), dots, x_(N,1);
    1, x_(1,2), x_(2,2), dots, x_(N,2);
    dots.v, dots.v, dots.v, dots.v, dots.v ;
    1, x_(1,N), x_(2,N), dots, x_(N,2);
  ) = mat(
    bar, bar, bar, bar, bar;
    1, x_1, x_2, dots, x_N;
    bar, bar, bar, bar, bar;
  )
$

In this case the estimator for the regression parameters is\
#bluebox[$
  hat(beta) = (X^T X)^(-1) X^T y med .
$ <eq:multiple-lr-est>]
Its variance-covariance matrix is #footnote[this can be easily derived by using the identity $Var(A x) = A Var(x) A^T$, and using the assumption of homoscedasticity to write $Var(epsilon) = sigma^2 bb(1)_(p+1)$.]\
#bluebox[
$
  Var(beta) = sigma^2 X^T X med .
$ <eq:multiple-lr-var>]

== Assumptions

Up until this point I haven't gone into much detail about the assumptions we have made. 
I've just blitzed through the derivation of the estimators. Here we enumerate the assumptions and give them fancy names which I think were popularised by econometrics.
Memorising the assumptions is important because they are almost always violated. 
If they're violated a little then you're probably fine proceeding as usual, but when they're violated a lot we need to introduce ways to fix things. 

#bluebox[
  #underline[*Linear Regression Assumptions*]
  + *Linearity*: the model is linear in the variate and parameters.
  + *Random sampling*: the data $(x_i, y_i)$ are i.i.d., ensuring that the sample is representative of the population.
  + *No perfect multicollinearity*: the $p$ covariates are linearly independent. This imposes $"Rank"(X) = p$. 
  + *Weak exogeneity*: no information loss in $Y$ when conditioned on $X$, $EE[epsilon|X] = 0$.
  + *Homoscedasticity* the variance of errors is constant across all values of $X$.
  + *No autocorrelation*: errors are uncorrelated, $EE[epsilon_i, epsilon_j] = 0 space forall space i != j.$.
  + *Errors follow a distribution (optional)*: here we assumed Gaussian, but they could've been $t$-distributed
  + *Model specification*: basically "the model is correct". This assumption is often violated if for example there are additional features which have not been included in the model.
]

== Gauss Markov theorem

One of the most famous results is that the estimator #eref(<eq:multiple-lr-est>) is the _best linear unbiased estimator (BLUE)_, where best means lowest variance. The derivation is pretty straightforward so I will present it here.
First we define an arbitrary linear estimator of $beta$ as an estimator of the form 
$
  tilde(beta) = A y med ,
$
where $A in RR^((p + 1) times N)$. 
If it's unbiased then,
$
  EE(tilde(beta)) = beta med .\
$ <eq:gauss-markov-1>
On the other hand substituting #eref(<eq:multiple-lr-model>) for $y$ and using the fact that $EE(epsilon) = 0$ yields
$
  EE(tilde(beta)) = A EE(X beta + epsilon) = A X beta med .
$ <eq:gauss-markov-2>
Combining eqs. @eq:gauss-markov-1 and @eq:gauss-markov-2 gives
$
  A X beta = beta => A X = bb(1)_(p + 1) med .
$ <eq:gauss-markov-3>
#Eref(<eq:gauss-markov-3>) motivates us to decompose $A$ as 
$
  A = (X^T X)^(-1) X^T + C med ,
$
where $C in RR^((p+1) times N)$ is in the null space of $X$, i.e., $C X = 0$.
The first term can't simply be $X^(-1)$ since we need a matrix with the shape $(p + 1) times N$, and $X^(-1)$ would be $N times (p + 1)$. 

The variance of $tilde(beta)$ can be written as 
$
  Var(tilde(beta)) &= Var(A(X beta + epsilon)) = Var(A epsilon) = A Var(epsilon) A^T \
  &= [(X^T X)^(-1) X^T + C] sigma^2 [(X^T X)^(-1) X^T + C]^T "(using Var"(epsilon) = sigma^2")" \
  &= sigma^2 { (X^T X)^(-1) + X^T X^(-1) X^T C^T + C X (X^T X)^(-1) + C C^T} #<eq:gauss-markov-4c> \
  &= Var(hat(beta)) + sigma^2 C C^T med . #<eq:gauss-markov-4d> 
$
To go from #eref(<eq:gauss-markov-4c>) to #eref(<eq:gauss-markov-4d>) I eliminated the cross terms in the middle via the fact that $C X = 0$ and rewrote the first term using #eref(<eq:multiple-lr-var>).
Since $C$ is a positive semi-definite matrix we have shown that $Var(tilde(beta))$ exceeds $Var(hat(beta))$ by a positive semi-definite matrix
#footnote[To verify that $C C^T$ is positive semi-definite simply write $v = C^T x$, then for any $x$ $abs(v)^2 = x^T C C^T x >= 0.$], $sigma^2 C C^T$.



= Consequences of violating Gauss Markov assumptions


== Weak exogeneity

Some terms:
#defenv[
  _Exogeneity_ is the assumption that measurement errors are uncorrelated with the covariate $x$. In other words, $Cov(x,epsilon) = 0.$ We often write $EE[epsilon|x] = 0$
]
#defenv[
  _Endogeneity_ refers to the errors in measurement of $Y$ being correlated with measurements of $x$. 
]

In this section we consider the single-variable model in #eref(<eq:reg-model>).
We have assumed that there are no errors in our observations of the covariate $x$, but it's possible there actually are errors. If we naiively use the OLS estimator for $hat(beta)$ how does the estimate relate to the true value?
Violation of weak exogeneity is sometimes referred to as errors-in-variables.
In OLS regression it leads to _attenuation bias_, where $hat(beta)$ becomes biased towards 0. 

First, let's arrive at the effect using intuition. The OLS estimator for  $beta$ is  $S_(x y) slash S_x$.
If there are no errors in the measurement of $x$ then the only thing obscuring our ability to see the true covariance between $x$ and $y$ are the errors in $y$ that we assume in OLS regression.
Adding errors to $x$ has the effect of reducing the observed covariance between $x$ and $y$, so we should expect that if we use the OLS estimator in this case, our estimate would be biased towards zero than the same estimator when used in the case when there are no errors in $x$.

Now some maths. Denote the true value of $x$ by $x^*$ and let the error in measurements of $x_*$ be $eta$.
The model is given by taking #eref(<eq:reg-model>) and replacing $x -> x_*$,
$
  y = beta_0 + x_* beta_1 + epsilon med ,
$
but since we can only measure $x = x_* + eta$ we have, in practice,
$
  y &= beta_0 + (x - eta) beta_1 + epsilon med #no-num \
  &= beta_0 + x beta_1 + (epsilon - beta_1 eta) \ 
  &equiv beta_0 + x beta_1 + tilde(epsilon) med,
$
where $tilde(epsilon) = epsilon - beta_1 eta$ is identified as the "new" residual, which is now correlated with $x$. 
The OLS estimator for $beta_1$ then converges to
$
  hat(beta)_1 = (sum_i (x_i - xbar)(y_i - ybar)) / (sum_i (x_i - xbar)^2) arrow Cov(x,y) / Var(x) = (sigma_(x_*)^2) / (sigma_(x_*)^2 + sigma_eta^2) thin beta_1,
$
which is less than or equal to $beta_1$. This effect is called _attenuation damping_. 
In deriving this expression I used the fact that we condition on the observed $x$ but are uncertain about the true value $x_*$ and the noise $eta$.
We have,
$
  Cov(x,y) 
  &= Cov(x_* + eta, beta_0 + beta_1 x_* + epsilon) #no-num \
  &= Cov(x_*, beta_1, x_*) + Cov(eta, beta_1 x_*) + Cov(eta, epsilon) #no-num \
  &= beta_1 Var(x_*) + 0 + 0 equiv beta_1 sigma^2_(x*) #no-num
$


_Note:_ The first time I encountered this I was very confused about the meaning of $Cov(x,y)$ because I had the perspective that $x$ is not a random variable and $y$ is. 



#bibliography("ref.bib", title:"References")